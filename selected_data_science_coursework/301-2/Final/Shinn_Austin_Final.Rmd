---
title: "Final Project"
subtitle: "Data Science II (STAT 301-2)"
author: "Austin Shinn"
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
---


```{r include=FALSE}
options(warn=-1)
# loading packages and reading in data
library(tidymodels)
library(tidyverse)
library(janitor)
library(xgboost)
library(ranger)
library(kknn)
library(skimr)

```

I ended up not using the original data set that I found on Kaggle, as closer inspection on the Happiness report website showed that some numbers didn't make sense for what they were supposed to be; for example, for the 2019 data, social support is supposed to be the average of a binary variable, but the maximum values in the data hover around 1.6. Because of this, I just ended up finding the data again on the official reports website.


```{r}
# 2020 report data (from 2019)
data_pt1 <- read_csv("data/unprocessed/WHR20_DataForFigure2.1.csv") %>%
  clean_names() %>%
  mutate(year = 2019)

# data from 2019 report, with data from 2005-2018
data_pt2 <- read_csv("data/unprocessed/Chapter2OnlineData-1.csv") %>%
  clean_names()

```


```{r}
# selecting relevant columns for model
data1 <- data_pt1 %>% select(c("country_name", "ladder_score", "logged_gdp_per_capita", "social_support", "healthy_life_expectancy", "freedom_to_make_life_choices", "perceptions_of_corruption", "year"))

data2 <- data_pt2 %>% select(c("country_name", "life_ladder", "log_gdp_per_capita", "social_support", "healthy_life_expectancy_at_birth", "freedom_to_make_life_choices", "perceptions_of_corruption", "year")) %>%
  rename("ladder_score" = "life_ladder",
  "logged_gdp_per_capita" = "log_gdp_per_capita",
         "healthy_life_expectancy" = "healthy_life_expectancy_at_birth"
         )

# combining the data sets, adding 2019 data to the existing data, making the final working data set
data <- full_join(data1, data2)

ggplot(data, aes(ladder_score)) +
  geom_histogram()

skim(data)

# already logged
ggplot(data, aes(logged_gdp_per_capita)) + 
  geom_histogram()

# left skew
ggplot(data, aes(social_support)) + 
  geom_histogram()

# left skew
ggplot(data, aes(healthy_life_expectancy)) + 
  geom_histogram()

#left skew
ggplot(data, aes(freedom_to_make_life_choices)) + 
  geom_histogram()


# left skew
ggplot(data, aes(perceptions_of_corruption)) + 
  geom_histogram()

```

From a quick look at the data, it looks pretty good - if the columns aren't complete, they are at least about 95%+ complete. To fill in the missing values, K-nearest neighbors imputation will be used as recommended by the textbook for all the methods I'll be using (https://www.tmwr.org/pre-proc-table.html). Several of the predictors will also have to be log transformed to account for left or right skew, and will be normalized.


```{r}
# splitting data into 80/20, stratifying by ladder_score to ensure there is data from different happiness scores
split <- initial_split(data, prop = .8, strata = ladder_score)

happy_train <- training(split)
happy_test <- testing(split)

vfold <- vfold_cv(happy_train, v = 10, repeats = 3, strata = ladder_score)
```

From here, I split the data into an 80/20 proportion, since there were enough observations to afford making the test set a smaller proportion. V-fold cross validation was employed, with 10 folds and 3 repetitions.

```{r}
# prepping and baking
recipe <- happy_train %>%
  # all predictors except country name and year of observation
  # using KNN imputation to fill in missing values
  recipe(ladder_score ~ logged_gdp_per_capita + social_support + healthy_life_expectancy + freedom_to_make_life_choices + perceptions_of_corruption) %>%
  step_knnimpute(all_predictors(), neighbors = 3) %>%
  step_log(social_support, healthy_life_expectancy, freedom_to_make_life_choices, perceptions_of_corruption, base = 10, offset = 5e-13) %>%
  step_normalize(all_predictors())

recipe %>%
  prep(training = happy_train) %>%
  bake(new_data = NULL) %>%
  view()

```

For the recipe, I first imputed using k-nearest neighbors to fill in the missing values. Then I log transformed the variables (except for gdp per capita, which was already logged). After that, the predictor variables were all normalized.


```{r}
rf_model <- rand_forest(mode = "regression",
                        min_n = tune(),
                        mtry = tune()) %>% 
  set_engine("ranger")

bt_model <- boost_tree(mode = "regression",
                       mtry = tune(),
                    min_n = tune(),
                    learn_rate = tune()) %>%
  set_engine("xgboost")

knn_model <- nearest_neighbor(mode = "regression",
                              neighbors = tune()) %>%
  set_engine("kknn")
```

Models were chosen, the same ones as in lab 7.

```{r}
rf_params <- parameters(rf_model) %>% 
  update(mtry = mtry(c(2,5)))

rf_grid <- grid_regular(rf_params, levels = 5)

bt_params <- parameters(bt_model) %>%
  update(mtry = mtry(c(2,5)),
         learn_rate = learn_rate(c(-5, -.2)))

bt_grid <- grid_regular(bt_params, levels = 5)

knn_params <- parameters(knn_model)

knn_grid <- grid_regular(knn_params, levels = 5)
```

Parameters were set based on the number of predictors, with the tree models spanning from 2 to all of the predictors in a tree.

```{r}
rf_workflow <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(recipe)

 bt_workflow <- workflow() %>%
  add_model(bt_model) %>%
  add_recipe(recipe)

knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(recipe)
```

```{r, eval = FALSE}
rf_tune <- rf_workflow %>%
  tune_grid(resamples = vfold,
            grid = rf_grid)

bt_tune <- bt_workflow %>%
  tune_grid(resamples = vfold,
            grid = bt_grid)

knn_tune <- knn_workflow %>%
  tune_grid(resamples = vfold,
            grid = knn_grid)

save(rf_tune, bt_tune, knn_tune, file = "data/final_tune_grids.rda")

```

Workflows and tuning grids were then created. The tuning grids were run and then saved to avoid rerunning in subsequent knittings.

```{r}
load("data/final_tune_grids.rda")

autoplot(rf_tune, metric = "rmse")

autoplot(bt_tune, metric = "rmse")

autoplot(knn_tune, metric = "rmse")
```

Random forest model: it appears that a smaller minimal node size and either 3 or 4 randomly selected predictors generated the lowest RMSE.

Boosted tree model: a higher learning rate was the most important factor. The other parameters didn't visibly change the RMSE.

K-Nearest neighbors: the model did best at 8 neighbors, with having both less and more neighbors increasing the RMSE.

```{r}
show_best(rf_tune, metric = "rmse")
show_best(rf_tune, metric = "rsq")

show_best(bt_tune, metric = "rmse")
show_best(bt_tune, metric = "rsq")

show_best(knn_tune, metric = "rmse")
show_best(knn_tune, metric = "rsq")

```
From the RMSE of the models, it appears that the random forest model has the best metrics. With R^2, the difference is smaller, but it still is the best out of the other methods. For a score that is out of 10, an RMSE of around .4 is not amazing, but pretty accurate.

```{r}
rf_workflow_tuned <- rf_workflow %>% 
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

rf_results <- fit(rf_workflow_tuned, happy_train)

final_metric <- metric_set(rmse, rsq)

predict(rf_results, new_data = happy_test) %>% 
  bind_cols(happy_test %>% select(ladder_score)) %>% 
  final_metric(truth = ladder_score, estimate = .pred)

save(data, file = "data/processed_data.rda")

```

When we run the final model on the test set, we find that the model performs comparably to the test set, showing that it is not overfitted to the training data, and the model is reasonably powerful for predicting happiness score of a nation given the predictors.

```{r include=FALSE}
options(warn=0)
```