---
title: "L05 Yardsticks for Classification"
subtitle: "Data Science II (STAT 301-2)"
author: "Austin Shinn"
output:
  html_document:
    toc: true
    toc_float: true
    highlight: "tango"
---

## Overview

The goal of this lab is to complete a model workflow for a classification machine learning problem, compare the performance of three models, and evaluate the chosen model with several classification metrics.

This lab covers material up to and including [9. Judging model effectiveness](https://www.tmwr.org/performance.html) from [Tidy Modeling with R](https://www.tmwr.org/).

### Load Packages & Set a Seed

```{r, message=FALSE, warning=FALSE}
# Load packages here!
library(tidyverse)
library(tidymodels)
library(ranger)
# Set seed here!
set.seed(77)
```

## Tasks

### Task 1

For this lab, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

```{r}
titanic_data <- read.csv('data/titanic.csv')

titanic_data$survived <- factor(titanic_data$survived, levels = c("Yes", "No"))

titanic_data$pclass <- factor(titanic_data$pclass, levels = c(1,2,3), labels = c("1st", "2nd", "3rd"))
```

### Task 2

Using the full data set, explore/describe the distribution of the outcome variable `survived`.

```{r}
# total counts of survivors and fatalities
titanic_data %>%
  group_by(survived) %>%
  summarise(count = n()) 

# proportions of survivors among groups
titanic_data %>%
  mutate(survived = ifelse(survived == "Yes", 1, 0)) %>%
  group_by(survived) %>%
  summarise(count = n()) %>%
  summarise(prop = count/sum(count))
```

From the dataset, there were 342 survivors, corresponding to a 38.4% survival rate.


### Task 3

Split the data! ***(Make sure to set a seed)***. Use stratified sampling. You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. Perform a skim of the training data and note any potential issues such as missingness.

Why is it a good idea to use stratified sampling for this data?

```{r}
# split data into 80/20
titanic_split <- initial_split(data = titanic_data, prop = 0.80, strata = survived)

# training and testing
titanic_training <- training(titanic_split)
titanic_testing <- testing(titanic_split)

# skim data
skimr::skim(titanic_training)
```
There are a lot of missing data for cabin and age; most predictors are characters so hard to tell anything from skimming, it just says how long the strings are. Cabin being missing shouldn't be a huge deal since it probably wouldn't be practical in a model. Age may be an issue.

Stratified sampling is better since we are trying to understand 2 different groups, survivors and fatalities, so we don't want to end up with too little of one group from random sampling.


### Task 4

Looking ahead, we plan to train two random forest models and a logistic regression model for this problem. We begin by setting up recipes for each of these approaches.


#### Logistic Regression Recipe

Using the training data, create and store a recipe setting `survived` as the outcome and using the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to **dummy** encode categorical predictors. Finally, include interactions between:

-   Sex and passenger fare, and
-   Age and passenger fare.

```{r}
log_reg_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_training) %>%
  step_impute_linear(age) %>%
  step_dummy(sex, pclass) %>%
  step_interact(~ starts_with("sex"):fare + age:fare)
```

#### Tree-Based Recipe

Using the training data, create and store a recipe setting `survived` as the outcome and using the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

Recall that there were missing values for `age`. To deal with this, add an imputation step using `step_impute_linear()`. Next, use `step_dummy()` to **one-hot** encode categorical predictors.

Tree-based methods naturally search out interactions, meaning that we don't need to specify any (of course, there are exceptions). Tree-based methods typically work better using one-hot encoding instead of traditional dummy coding; this also has to do with the fact that they are empirically driven models, not mechanistic.

```{r}
tree_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_training) %>%
  step_impute_linear(age) %>%
  step_dummy(sex, pclass, one_hot = TRUE) 
```

### Task 5

Specify a **logistic regression** model for classification using the `"glm"` engine. Then create a workflow. Add your model and the appropriate recipe. Finally, use `fit()` to apply your workflow to the **training** data.

***Hint: Make sure to store your results when you apply the workflow. You'll need them later on.***

```{r}
# set engine to logistic
model <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

# create workflow
log_workflow <- workflow() %>%
  add_model(model) %>%
  add_recipe(log_reg_recipe) 

# fit the logistic regression
log_fit <- fit(log_workflow, titanic_testing)
```

### Task 6

**Repeat Task 5**, but this time specify a random forest model for classification using the `"ranger"` engine and the appropriate recipe. *Don't specify values for tuning parameters manually;* allow the function(s) to use the default values.

***Hint: Make sure to store your results when you apply the workflow. You'll need them later on.***

Using `?rand_forest`, read the function documentation to find out what default values `ranger` uses for `mtry`, `trees`, and `min_n`. What are the defaults in this case?

```{r}
tree_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

# create workflow
tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(tree_recipe) 

# fit random forest
tree_fit <- fit(tree_workflow, titanic_testing)
```

The defaults are: 500 trees, mtry is the square root of the number of variables, and min_n is 1

### Task 7

**Repeat Task 6**, but this time choose values that you think are reasonable for each of the three tuning parameters (`mtry`, `trees`, and `min_n`).

***Hint: Make sure to store your results when you apply the workflow. You'll need them later on.***

```{r}
tree_model2 <- rand_forest(mtry = 4, trees = 1000, min_n = 3) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# create workflow
tree_workflow2 <- workflow() %>%
  add_model(tree_model2) %>%
  add_recipe(tree_recipe) 

# fit random forest
tree_fit2 <- fit(tree_workflow2, titanic_testing)
```

### Task 8

Now you've fit three different models to your training data:

1.  A logistic regression model
2.  A random forest model with default tuning parameters
3.  A random forest model with custom tuning parameters

Use `predict()` and `bind_cols()` to generate predictions using each of these 3 models and your **testing** data. Then use the *accuracy* metric to assess the performance of each of the three models.

Which model makes the best predictions using the testing data? How do you know?

```{r}
titanic_metrics <- metric_set(accuracy)

# fit predictions for logistic, default random forest, and custom random forest
log_pred <- predict(log_fit, new_data = titanic_testing %>% select(sex, age, pclass, sib_sp, parch, fare)) %>%
  bind_cols(titanic_testing %>% select(survived))

tree_pred <- predict(tree_fit, new_data = titanic_testing %>% select(sex, age, pclass, sib_sp, parch, fare)) %>%
  bind_cols(titanic_testing %>% select(survived))

tree_pred2 <- predict(tree_fit2, new_data = titanic_testing %>% select(sex, age, pclass, sib_sp, parch, fare)) %>%
  bind_cols(titanic_testing %>% select(survived))

# calculate and display metrics
titanic_metrics(log_pred, truth = survived, estimate = .pred_class)
titanic_metrics(tree_pred, truth = survived, estimate = .pred_class)
titanic_metrics(tree_pred2, truth = survived, estimate = .pred_class)

```
The second tree had the best accuracy by far, with an accuracy of 96.6%.

### Task 9

With the model you chose in Task 8, create a confusion matrix using the **testing** data.

Explain what this is in your own words. Interpret the numbers in each category.

```{r}
conf_mat(tree_pred2, truth = survived, estimate = .pred_class)
```

It's a matrix comparing how the predictions line up with the true values. There are 2 values, yes and no for pred and truth, and the number where they intersect is how many there are. For example, there are 65 times where the model predicted true when it really was true, and 106 times when it predicted no when it was no.

### Task 10

With the model you chose in Task 8, use `predict()` and `bind_cols()` to create a tibble of predicted class probabilities and actual true outcomes. Note that this will require using the `type` argument of `predict()`. You should be using the **testing** data.

Explain what these class probabilities are in your own words.

```{r}
tree_pred2 <- predict(tree_fit2, new_data = titanic_testing %>% select(sex, age, pclass, sib_sp, parch, fare), type = "prob") %>%
  bind_cols(titanic_testing %>% select(survived))

tree_pred2
```

The predictions are the conditional probability for yes and no given the predictors. The higher conditional probability is the prediction.

These probabilities refer to the conditional probability assigned to each class “Yes” or “No” for observations given all other factors. The class predicted is the one with the higher probability.


### Task 11

With the model you chose in Task 8, use `roc_curve()` and `autoplot()` to create a receiver operating characteristic (ROC) curve.

Use `roc_auc()` to calculate the area under the ROC curve.

```{r}
two_class_curve <- roc_curve(tree_pred2, survived, .pred_Yes)

# calculate area under curve
roc_auc(tree_pred2, survived, .pred_Yes)

# plots the curve
autoplot(two_class_curve)

```

### Task 12

The area under the ROC curve is a measure of how well the model predictions are able to separate the data being tested into classes/groups. [(See here for a more detailed explanation)](http://gim.unmc.edu/dxtests/roc3.htm).

Interpret the AUC for your model.

The area under the curve is .9693, which indicates that the model is very good at telling whether or not a passenger survived. This means that the for random pairs drawn from each group, the model was able to classify both at a 96.9% accuracy.